---
title: "A Stunning Breakthrough in Quantum Machine Learning"
datePublished: Mon Jul 26 2021 20:44:49 GMT+0000 (Coordinated Universal Time)
cuid: cl9y38vl7002d77nv1m6t5meg
slug: a-stunning-breakthrough-in-quantum-machine-learning-31fd004d2230
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1667299786469/nXY5scNmno.jpeg

---

#### This is a brief summary of some sections of the research paper **A Novel** *Autonomous (Quantum) Perceptron Model* by **Alaa Sagheer, Mohammed Zidan, and Mohammed M. Abdelsamea**

*Support my writing*

**Join Medium.com as a member** through the link below.

[**https://thomascherickal.medium.com/membership**](https://thomascherickal.medium.com/membership)

#### The official citation:

Sagheer, Alaa & Zidan, Mohammed & Abdelsamea, Mohammed. (2019). A Novel Autonomous Perceptron Model for Pattern Classification Applications. Entropy. 21. 763. 10.3390/e21080763.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1667299782885/L8BfcbmoIh.jpeg)

In many ways, I was delighted to find that quantum computation and machine learning were a natural choice for interesting research (sparking a potential career in Quantum Machine Learning as a research scientist — still waiting).

I could not get rid of the realization that if we could implement the cost function of an optimization problem, and feed data in, one instance (1 row) at a time, using quantum computing evolution, the quantum machine algorithm could be adjusted to resolve to the optimal solution. In parallel, using one single neuron.

**As far as the single neuron evolution operator is concerned, these scientists: Alaa Sagheer, Mohammed Zidan & Mohammed Abdelsamea seem to have achieved exactly that!**

Of course, I further hoped that this model could be extended in the general case to quantum deep learning algorithms and applications in the future — but I digress.

#### The Knock-Out Punch

I came across this line from the article:

> “The APM is designed with an optimal neural structure of only one single neuron to classify  
> nonlinear separable datasets.”

***Excuse me***?

Isn’t all of the non-linear dynamical systems directed towards optimization and classification of non-linear system spaces? Isn’t that the **holy grail as far as industrial applications are concerned?**

And this was the performance test in many of the standard data sets that we AI researchers use — the UCI Machine Learning Dataset Library at Berkeley. Remember, this is one single neuron, it can be implemented on classical computers through simulation, and the competition was 3 layer feed-forward back-propagation neural networks with numerous classical artificial neurons. The results are shown below.

### Always Cite Your Sources

From:

*Sagheer, Alaa & Zidan, Mohammed & Abdelsamea, Mohammed. (2019). A Novel Autonomous Perceptron Model for Pattern Classification Applications. Entropy. 21. 763. 10.3390/e21080763.*

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1667299784631/y8MId_j8c.png)

Table 8. Time complexity comparison (in seconds) among the proposed APM classifier and other  
standard classifiers.

What struck me the most was not just the speed, but also that exponentially smaller amounts of data were used to train this single neuron. Anybody working in data science or neural networks knows that the biggest bane of data science is — data! To be precise: **data munging and wrangling**. The sheer manual labor behind that task has kept me far, far away from data science. I’d much rather work on this — quantum-inspired models that perform exponentially better on subsets of the data.

ANNs are great, but they have one problem — they need data. Tons of data. So much that the best ANN research work can only be done at FAAMGA and select research departments because collecting the data for one model can take years and petabytes of memory. Not an exaggeration. And after collection, enjoy data wrangling, because that will be 96% of your work. But if you know your data, you can get very good results on neural networks, especially with deep learning.

**This model seems to have overcome that!**

Now we get state-of-the-art results after exposure to very limited amounts of data!

Obviously, the implications are huge.

### Implementation

The authors stated unequivocally that this ‘novel’ AQM can be simulated on a classical computer.

However, I have not been able to find an implementation anywhere.

This means I need to do it myself — or engage researchers in this technology — may be the original paper authors — to collaborate. Because every environment causes side issues that you will not be able to replicate elsewhere.

I’ll leave the question open to you. You have my Medium link. Get in touch with me and let’s see what we can do.

Just to whet your appetite even more, some more data from the paper:

From the paper:

> Simply, although the APM model has only a single neuron, it possesses the same computational power of three layers of the feed-forward neural network with the neuron topology 2n − 4 − 2, where n is the number of neurons.  
> ... It is clear that the APM model that has only a single neuron performs the same nonlinear mapping that can be  
> performed by the three-layer classical neural network.

Then three or more APMs along with quantum entanglement, superposition, and interference — running in parallel — should be able to crack the pinnacle problem of quantum neural networks — **deep learning architectures! Because one single APM has the computational capacity of three feedforward neural networks layers! One quantum neuron has the ability to replace hundreds or thousands of classical neurons!**

To say the very least, I am very excited!

Interested? You can reach me on this Medium profile. And do check out my profile website: [https://thomascherickal.com](https://thomascherickal.com).

I look forward to working with you. Sincerely.

The paper referred to above:

*Sagheer, Alaa & Zidan, Mohammed & Abdelsamea, Mohammed. (2019). A Novel Autonomous Perceptron Model for Pattern Classification Applications. Entropy. 21. 763. 10.3390/e21080763.*

A personal request to the researchers — if you could send me the source code and the details of the implementation you used to my email ID: **thomascherickal@gmail.com**, I would be a very happy man. In the spirit of open source and the Creative Commons license.

Thanks for reading!

Do clap if you’re interested, do contact me to (possibly) work further in this field.

Cheers!

Photo by [FLY:D](https://unsplash.com/@flyd2069?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

#### Support My Writing

**Support my writing.** **Join Medium** to read the best content on the web through the link below and 50% of your membership fee supports me every month. A sincere thanks to all my wonderful subscribers who help me and support my writing every month.

Join Medium with my referral link below — Thomas Cherickal

[**https://thomascherickal.medium.com/membership**](https://thomascherickal.medium.com/membership)

God bless.

### Coffee

If you enjoyed this article, do buy me a coffee every month for 2 USD:

[https://ko-fi.com/thomascherickal](https://ko-fi.com/thomascherickal)