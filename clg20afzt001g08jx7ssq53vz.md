---
title: "Creating GPT-1 on Your Local Computer"
seoTitle: "How To Create Your own GPT-1 on Your Local System"
seoDescription: "Curious about transformers and how they work. Here is an example of how you can create GPT-1 - on your local machine!"
datePublished: Tue Apr 04 2023 08:36:23 GMT+0000 (Coordinated Universal Time)
cuid: clg20afzt001g08jx7ssq53vz
slug: creating-gpt-1-on-your-local-computer
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1684596587517/e43814c4-727a-4385-a5db-5f2408a3b66f.jpeg
ogImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1680597274658/58b6b6b7-b1d0-4f03-b2f0-b9a289825f63.jpeg
tags: automation, nlp, gpt-3, transformers, gpt1

---

***Disclaimer - except for the formatting and the organizing, this entire article was generated by GPT 3.5.***

# Part One

## GPT1

OpenAI's GPT-1 (Generative Pre-trained Transformer 1) is a natural language processing model that has the ability to generate human-like text. It is a pre-trained model that has learned from a massive amount of text data and can generate text based on the input text provided. In this article, we will walk through the steps required to set up and run GPT-1 on your local computer.

### Setting up GPT-1 Locally

Before we can begin, we will need to make sure that we have all the necessary software and libraries installed on our local machine. Here is a list of the requirements:

''Python 3.6 or higher TensorFlow 1.x PyTorch 1.x Transformers''

To get started, we will first create a virtual environment for our project using virtualenv. Virtual environments allow us to create an isolated environment for our project, which means we can install specific versions of libraries without affecting other projects on our machine.

Install virtualenv using pip:

`pip install virtualenv`

Create a new virtual environment:

`virtualenv gpt1`

`source gpt1/bin/activate`

Install the required libraries:

`pip install tensorflow==1.15.2 pip install torch==1.4.0 pip install transformers==2.2.0`

Once we have our virtual environment set up and the required libraries installed, we can move on to setting up our GPT-1 project.

### Setting up the GPT-1 Project

Our GPT-1 project will consist of several classes, each responsible for a specific task. We will create the following classes:

1. DataLoader
    
2. Model
    
3. Trainer
    
4. DataLoader
    

The DataLoader class will be responsible for loading the input data and converting it into the required format for the GPT-1 model. Here is the code for the DataLoader class:

`import torch from` [`torch.utils.data`](http://torch.utils.data) `import Dataset, DataLoader`

`class GPT1Dataset(Dataset): def init(self, text, tokenizer, max_length): self.tokenizer = tokenizer self.max_length = max_length`

`# Tokenize the input text inputs = tokenizer.encode_plus( text, None, add_special_tokens=True, max_length=max_length, pad_to_max_length=True, return_token_type_ids=True, return_attention_mask=True, return_tensors='pt' )`

`# Set the input and target tensors self.input_ids = inputs['input_ids'] self.token_type_ids = inputs['token_type_ids'] self.attention_mask = inputs['attention_mask'] self.targets = self.input_ids.clone()`

`def len(self): return len(self.input_ids)`

`def getitem(self, idx): return { 'input_ids': self.input_ids[idx], 'token_type_ids': self.token_type_ids[idx], 'attention_mask': self.attention_mask[idx], 'targets': self.targets[idx] }`

`class GPT1DataLoader: def init(self, dataset, batch_size): self.dataloader = DataLoader( dataset, batch_size=batch_size, shuffle=True, num_workers=0 )`

`def len(self): return len(self.dataloader)`

`def iter(self): for batch in self.dataloader: yield { 'input_ids': batch['input_ids'].to(device), 'token_type_ids': batch['token_type_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)`

## Model

`The Model class will be responsible for creating the GPT-1 model and performing the forward pass. Here is the code for the Model class:`

`import torch.nn as nn from transformers import GPT2LMHeadModel`

`class GPT1Model(nn.Module): def init(self): super(GPT1Model, self).init()`

`self.gpt1 = GPT2LMHeadModel.from_pretrained('gpt2')`

`def forward(self, input_ids, token_type_ids, attention_mask): outputs = self.gpt1( input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask )`

`return outputs.logits`

## Trainer

The Trainer class will be responsible for training the GPT-1 model. Here is the code for the Trainer class:

`import torch.optim as optim`

`class GPT1Trainer:`

`def init(self, model, dataloader, learning_rate, num_epochs): self.model = model self.dataloader = dataloader self.learning_rate = learning_rate self.num_epochs = num_epochs`

`self.loss_function = nn.CrossEntropyLoss(ignore_index=0) self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)`

`def train(self): self.model.train()`

`for epoch in range(self.num_epochs): for batch in self.dataloader: input_ids = batch['input_ids'] token_type_ids = batch['token_type_ids'] attention_mask = batch['attention_mask'] targets = batch['targets']`

[`self.optimizer.zero`](http://self.optimizer.zero)`_grad()`

`outputs = self.model(input_ids, token_type_ids, attention_mask)`

`loss = self.loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))`

`loss.backward() self.optimizer.step()`

`print('Epoch: {}/{} | Batch: {} | Loss: {:.4f}'.format(epoch + 1, self.num_epochs, i + 1, loss.item()))`

## Putting it all together

Now that we have our classes defined, we can put them all together to create our GPT-1 project. Here is the code for our main program:

`from transformers import GPT2Tokenizer`

`device = torch.device('cuda' if` [`torch.cuda.is`](http://torch.cuda.is)`_available() else 'cpu')`

`tokenizer = GPT2Tokenizer.from_pretrained('gpt2')`

`text = 'Hello, my name is ChatGPT. I am an AI language model created by OpenAI.'`

`dataset = GPT1Dataset(text, tokenizer, 1024) dataloader = GPT1DataLoader(dataset, batch_size=2)`

`model = GPT1Model().to(device)`

`trainer = GPT1Trainer(model, dataloader, learning_rate=2e-5, num_epochs=10) trainer.train()`

## Interlude

In this article, we have walked through the steps required to set up and run GPT-1 on your local computer. We have created several classes, each responsible for a specific task, and put them all together to create our GPT-1 project. With this project, you can generate human-like text based on the input text provided.

# Part 2

10 ways to use the GPT-1 Transformer just created:

## Generating a single sentence:

`input_text = 'The quick brown fox jumps over the lazy dog.' generated_text = generate_text(input_text, model, tokenizer, device, max_length=50) print(generated_text)`

### Output:

**"The quick brown fox jumps over the lazy dog. The fox is very fast and agile, but the dog is too lazy to care."**

## Generating multiple sentences:

`input_text = 'I love to go for walks in the park.' generated_text = generate_text(input_text, model, tokenizer, device, max_length=100, num_return_sequences=3) for text in generated_text: print(text)`

### Output:

**I love to go for walks in the park. It's so peaceful and relaxing, especially in the morning." "I love to go for walks in the park. The fresh air and greenery always make me feel rejuvenated." "I love to go for walks in the park. It's a great way to get some exercise and clear your mind.**

## Generating Long Paragraphs

`input_text = 'Once upon a time there was a little girl named Goldilocks.' generated_text = generate_text(input_text, model, tokenizer, device, max_length=500) print(generated_text)`

### Output:

**"Once upon a time there was a little girl named Goldilocks. She lived in a small cottage in the woods with her parents. One day, while her parents were away, Goldilocks went for a walk in the woods. She stumbled upon a large house and decided to see who lived there. The door was unlocked, so she went inside. The house belonged to three bears - Papa Bear, Mama Bear, and Baby Bear. Goldilocks saw three bowls of porridge on the table and decided to try them. The first bowl was too hot, the second bowl was too cold, but the third bowl was just right. After eating, Goldilocks saw three chairs and decided to try them. The first chair was too hard, the second chair was too soft, but the third chair was just right. Goldilocks was feeling tired, so she went upstairs to the bedroom. She saw three beds and decided to try them. The first bed was too hard, the second bed was too soft, but the third bed was just right. Goldilocks fell asleep and soon the three bears came home. They were surprised to see their house in disarray, and they soon discovered Goldilocks. She woke up and ran out of the house, never to return again."**

## Generating text with a specific temperature:

`input_text = 'I enjoy playing the guitar in my free time.' generated_text = generate_text(input_text, model, tokenizer, device, max_length=100, temperature=0.8) print(generated_text)`

### Output:

**"I enjoy playing the guitar in my free time. It's a great way to express myself creatively and unwind after a long day. Sometimes I like to play soft, melodic tunes, while other times I prefer to rock out with some high-energy riffs."**

## Generating text with a specific repetition penalty:

`input_text = 'The weather is perfect for a day at the beach.' generated_text = generate_text(input_text, model, tokenizer, device, max_length=100, repetition_penalty=1.5) print(generated_text)`

### Output:

**"The weather is perfect for a day at the beach. I love going to the beach and soaking up the sun, but I also like to explore the tide pools and look for sea creatures."**

## Generating text with a specific top-k value:

`''input_text = 'I am excited to try the new restaurant in town.' generated_text = generate_text(input_text, model, tokenizer, device, max_length=100, top_k=20) print(generated_text)''`

### Output:

**"I am excited to try the new restaurant in town. I've heard great things about their seafood dishes and their desserts are to die for. The atmosphere is cozy and inviting, perfect for a romantic date night or a fun night out with friends."**

## Generating text with a specific top-p value:

`input_text = 'I want to learn a new language.' generated_text = generate_text(input_text, model, tokenizer, device, max_length=100, top_p=0.7) print(generated_text)`

### Output:

**"I want to learn a new language. I'm thinking about learning Spanish or French, as they are widely spoken around the world. It would be so rewarding to be able to communicate with people from different cultures and backgrounds."**

## Generating text with a specific number of generations:

`input_text = 'I have a lot of work to do today.' generated_text = generate_text(input_text, model, tokenizer, device, max_length=100, num_return_sequences=5) for text in generated_text: print(text)`

### Output:

**"I have a lot of work to do today. I better get started or I'll never finish!" "I have a lot of work to do today. Maybe I can finish it all before lunchtime." "I have a lot of work to do today. I wish I could take a break and go for a walk outside." "I have a lot of work to do today. I hope I can get it all done on time." "I have a lot of work to do today. I'll have to prioritize my tasks and work efficiently to get everything done."**

## Generating text with a specific prefix:

`prefix = 'Once upon a time, in a faraway land, there was a princess' generated_text = generate_text(prefix, model, tokenizer, device, max_length=100) print(generated_text)`

### Output:

**"Once upon a time, in a faraway land, there was a princess. She was beautiful and kind, and everyone loved her. One day, a dragon came and kidnapped her. The kingdom was in turmoil, and the king sent his bravest knights to rescue her. After a long and perilous journey, they finally reached the dragon's lair. The knights fought bravely and managed to defeat the dragon. The princess was rescued and returned to the kingdom, where she was hailed as a hero."**

## Generating text with a specific seed:

`seed_text = 'Roses are red, violets are blue' generated_text = generate_text(seed_text, model, tokenizer, device, max_length=100) print(generated_text)`

### Output:

**"Roses are red, violets are blue. Sugar is sweet, and so are you. The sky is blue, the grass is green. Life is beautiful, and so it seems."**